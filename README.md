# ğŸ¯ YOLO Object Detection with VLM Assistant

A real-time object detection application combining YOLOv11 with Vision Language Model (VLM) capabilities, optimized for NVIDIA Jetson Orin NX development boards.

![Python](https://img.shields.io/badge/python-3.10-blue.svg)
![JetPack](https://img.shields.io/badge/JetPack-6.0-green.svg)
![CUDA](https://img.shields.io/badge/CUDA-Enabled-orange.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)


## ğŸ“¹ Demo

Hereâ€™s a demo video of the project in action:

ğŸ‘‰ ![Output Demo](output.mp4)



## ğŸš€ Features

- **Real-time Object Detection**: YOLOv11 powered person detection with live webcam streaming
- **VLM Integration**: Local Ollama-based Vision Language Model for intelligent scene analysis
- **GPU Toggle**: Dynamic switching between CUDA and CPU inference
- **Web Interface**: Professional Flask-based dashboard with real-time controls
- **WebSocket Support**: Live status updates and real-time communication
- **Optimized for Jetson**: Specifically designed for NVIDIA Jetson Orin NX performance

## ğŸ—ï¸ System Requirements

### Hardware
- **NVIDIA Jetson Orin NX** (8GB recommended)
- **USB Webcam** or CSI camera
- **Storage**: Minimum 8GB free space
- **RAM**: 16GB memory

### Software
- **JetPack 6.0** (Ubuntu 22.04 base)
- **Python 3.10** with virtual environment
- **CUDA 12.x** (included with JetPack 6.0)

## ğŸ› ï¸ Installation Guide

### 1. System Preparation

First, ensure your Jetson Orin NX is running JetPack 6.0:

```
# Check JetPack version
sudo apt show nvidia-jetpack
```

### 2. Clone Repository

```
git clone https://github.com/12boopathi/Vision-Assistant-flask-vlm--jetson-orin.git
cd Vision-Assistant-flask-vlm--jetson-orin
```

### 3. Python 3.10 Virtual Environment Setup

Create and activate a Python 3.10 virtual environment 

```
# Install Python 3.10 venv if not available
sudo apt update
sudo apt install python3.10-venv python3.10-dev

# Create virtual environment
python3.10 -m venv venv

# Activate virtual environment
source venv/bin/activate

# Upgrade pip
python -m pip install --upgrade pip
```

### 4. Install Dependencies

```
# Install PyTorch with CUDA support for Jetson
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Install application requirements
pip install -r requirements.txt
```

### 5. Install and Configure Ollama

```
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start Ollama service
ollama serve &

# Pull Gemma VLM model (choose based on your memory)
ollama pull gemma3:4b
# Alternative: ollama pull llava:7b
```

## ğŸ“ Project Structure

```
yolo-vlm-detection/
â”œâ”€â”€ app.py                 # Main Flask application
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html        # Web interface
```

## ğŸš€ Quick Start

### 1. Activate Environment

```
cd Vision-Assistant-flask-vlm--jetson-orin
source venv/bin/activate
```

### 2. Start Ollama Service

```
# Start Ollama in background
ollama serve &
```

### 3. Run Application

```
python app.py
```

### 4. Access Web Interface

Open your browser and navigate to:
- **Local access**: `http://localhost:5000`
- **Network access**: `http://YOUR_JETSON_IP:5000`

## ğŸ’¡ Usage Guide

### Web Interface Controls

1. **Video Feed**: Live webcam stream with optional YOLO detection overlay
2. **Detection Toggle**: Enable/disable object detection in real-time
3. **GPU Toggle**: Switch between CUDA and CPU inference
4. **VLM Chat**: Interact with the vision language model
5. **Status Panel**: Monitor system performance and detection statistics

### VLM Chat Examples

Try these prompts with the VLM assistant:

- `"What do you see in the current frame?"`
- `"How many people are visible?"`
- `"Describe the scene in detail"`
- `"What objects can you identify?"`
- `"Is anyone wearing bright colors?"`

### Performance Optimization

#### For Maximum Performance (GPU):
```
# In app.py, ensure these settings:
detection_system.gpu_enabled = True
detection_system.model.to("cuda")
```

#### For Power Efficiency (CPU):
```
# Toggle to CPU mode via web interface or:
detection_system.gpu_enabled = False
detection_system.model.to("cpu")
```

## ğŸ”§ Configuration

### Camera Settings

Modify camera parameters in `app.py`:

```
# Camera resolution and FPS
self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
self.cap.set(cv2.CAP_PROP_FPS, 30)
```

### YOLO Model Configuration

Change YOLO model variant:

```
# Options: yolo11n.pt, yolo11s.pt, yolo11m.pt, yolo11l.pt, yolo11x.pt
self.model = YOLO("yolo11n.pt")  # Nano (fastest)
# self.model = YOLO("yolo11s.pt")  # Small (balanced)
```

### Ollama Model Configuration

Switch VLM models:

```
# In app.py
self.vlm_model = "gemma2-3b-vlm"    # Default
# self.vlm_model = "llava:7b"       # Alternative
# self.vlm_model = "llava:13b"      # More capable (requires more RAM)
```

## ğŸ”¬ Technical Details

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Browser   â”‚â—„â”€â”€â”€â”¤   Flask Server   â”‚â—„â”€â”€â”€â”¤  YOLO Detection â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚  - Video Stream â”‚    â”‚ - WebSocket API  â”‚    â”‚ - CUDA/CPU      â”‚
â”‚  - Control UI   â”‚    â”‚ - REST Endpoints â”‚    â”‚ - YOLOv11       â”‚
â”‚  - VLM Chat     â”‚    â”‚ - Template Renderâ”‚    â”‚ - Person Count  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Ollama VLM API  â”‚
                       â”‚                 â”‚
                       â”‚ - Gemma Model   â”‚
                       â”‚ - Image Analysisâ”‚
                       â”‚ - Local Inferenceâ”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ™ Acknowledgments

- **NVIDIA**: For Jetson platform and CUDA support
- **Ultralytics**: For YOLOv11 implementation
- **Ollama**: For local LLM infrastructure
- **Google**: For Gemma VLM models

```
